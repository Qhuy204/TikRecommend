# SASRec + PhoBERT Fusion Recommendation System

**Self-Attentive Sequential Recommendation vá»›i Vietnamese NLP Enhancement**

> Modern Deep Learning approach (2018, updated 2023+) cho Vietnamese E-commerce Recommendation

---

## ï¿½ Table of Contents

1. [Giá»›i thiá»‡u](#-giá»›i-thiá»‡u)
2. [LÃ½ thuyáº¿t SASRec](#-lÃ½-thuyáº¿t-sasrec)
3. [Kiáº¿n trÃºc Model](#-kiáº¿n-trÃºc-model)
4. [Loss Function](#-loss-function-bpr-loss)
5. [PhoBERT Integration](#-phobert-integration)
6. [HÆ°á»›ng dáº«n sá»­ dá»¥ng](#-hÆ°á»›ng-dáº«n-sá»­-dá»¥ng)
7. [References](#-references)

---

## ðŸŽ¯ Giá»›i thiá»‡u

### Problem Statement

Trong E-commerce, viá»‡c recommend sáº£n pháº©m dá»±a trÃªn **lá»‹ch sá»­ hÃ nh vi theo thá»© tá»± thá»i gian** (sequential behavior) ráº¥t quan trá»ng:

```
User clicks: Item_1 â†’ Item_2 â†’ Item_3 â†’ ?
                                         â†“
                              Predict: Item_4
```

### Why SASRec?

| Method | Year | Approach | Limitation |
|--------|------|----------|------------|
| Matrix Factorization | 2009 | Static latent factors | No temporal patterns |
| GRU4Rec | 2016 | RNN-based | Limited long-range |
| Caser | 2018 | CNN-based | Fixed window size |
| **SASRec** | **2018** | **Self-Attention** | **State-of-the-art** |
| BERT4Rec | 2019 | Bidirectional | More complex |

**SASRec advantages:**
- âœ… Captures **long-range dependencies** vá»›i O(1) path length
- âœ… **Parallelizable** - khÃ´ng sequential nhÆ° RNN
- âœ… **Lightweight** hÆ¡n BERT4Rec
- âœ… Proven performance trÃªn nhiá»u benchmarks

---

## ï¿½ LÃ½ thuyáº¿t SASRec

### Core Idea

**Self-Attentive Sequential Recommendation** sá»­ dá»¥ng Transformer self-attention Ä‘á»ƒ model user behavior sequences:

```
Traditional: User history â†’ RNN/LSTM â†’ Hidden state â†’ Predict next
SASRec:      User history â†’ Self-Attention â†’ Context-aware repr â†’ Predict next
```

### Key Components

#### 1. Item Embedding

Má»—i item Ä‘Æ°á»£c biá»ƒu diá»…n bá»Ÿi má»™t trainable embedding vector:

$$\mathbf{E} \in \mathbb{R}^{|V| \times d}$$

Trong Ä‘Ã³:
- $|V|$: Sá»‘ lÆ°á»£ng items (vocabulary size)
- $d$: Embedding dimension

#### 2. Positional Encoding

VÃ¬ self-attention khÃ´ng cÃ³ notion vá» order, cáº§n thÃªm positional information:

$$PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d})$$
$$PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d})$$

Sinusoidal encoding cho phÃ©p model há»c relative positions.

#### 3. Self-Attention Mechanism

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

Trong Ä‘Ã³:
- $Q = XW^Q$ (Query)
- $K = XW^K$ (Key)  
- $V = XW^V$ (Value)
- $\sqrt{d_k}$: Scaling factor Ä‘á»ƒ prevent gradient vanishing

#### 4. Causal Masking

**Critical!** Äá»ƒ prevent **information leakage** tá»« future items:

```
Sequence: [A, B, C, D, E]

Without mask:  A can see B, C, D, E  âŒ (cheating!)
With mask:     A can only see A      âœ…
               B can see A, B        âœ…
               C can see A, B, C     âœ…
```

Mask Ä‘Æ°á»£c implement báº±ng:
```python
causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)
scores = scores.masked_fill(causal_mask.bool(), -1e9)
```

#### 5. Multi-Head Attention

Cho phÃ©p model attend to different representation subspaces:

$$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O$$

Trong Ä‘Ã³: $\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$

#### 6. Feed-Forward Network

Position-wise FFN sau attention:

$$\text{FFN}(x) = \text{GELU}(xW_1 + b_1)W_2 + b_2$$

#### 7. Residual Connections & Layer Norm

```python
x = x + dropout(attention(layer_norm(x)))
x = x + dropout(ffn(layer_norm(x)))
```

---

## ðŸ— Kiáº¿n trÃºc Model

### Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    USER BEHAVIOR SEQUENCE                    â”‚
â”‚              [Item_1, Item_2, Item_3, ..., Item_n]          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ITEM EMBEDDING LAYER                      â”‚
â”‚                    E âˆˆ â„^(|V|+1 Ã— d)                        â”‚
â”‚                    (+1 for padding token)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  POSITIONAL ENCODING                         â”‚
â”‚                  PE âˆˆ â„^(max_len Ã— d)                       â”‚
â”‚                  Sinusoidal encoding                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              TRANSFORMER BLOCK Ã— L (default: 2)              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Layer Norm â†’ Multi-Head Self-Attention â†’ Dropout      â”‚ â”‚
â”‚  â”‚              (with causal mask)                         â”‚ â”‚
â”‚  â”‚                      â†“                                  â”‚ â”‚
â”‚  â”‚  Residual Connection (+)                                â”‚ â”‚
â”‚  â”‚                      â†“                                  â”‚ â”‚
â”‚  â”‚  Layer Norm â†’ Feed-Forward Network â†’ Dropout            â”‚ â”‚
â”‚  â”‚                      â†“                                  â”‚ â”‚
â”‚  â”‚  Residual Connection (+)                                â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FINAL LAYER NORM                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              GET LAST HIDDEN STATE                           â”‚
â”‚       h_n = hidden[last_valid_position]                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   PREDICTION LAYER                           â”‚
â”‚           score(item) = h_n Â· e_item                        â”‚
â”‚           (dot product with item embeddings)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Hyperparameters

| Parameter | Default | Description |
|-----------|---------|-------------|
| `embedding_dim` | 64 | Item embedding dimension |
| `num_attention_heads` | 2 | Number of attention heads |
| `num_transformer_blocks` | 2 | Number of transformer layers |
| `hidden_dim` | 128 | FFN hidden dimension |
| `max_seq_length` | 50 | Maximum sequence length |
| `dropout` | 0.2 | Dropout rate |

---

## ðŸ“ Loss Function: BPR Loss

### Bayesian Personalized Ranking (BPR)

SASRec sá»­ dá»¥ng **BPR Loss** cho implicit feedback learning:

$$\mathcal{L}_{BPR} = -\sum_{(u,i,j) \in D_S} \ln \sigma(\hat{x}_{ui} - \hat{x}_{uj})$$

Trong Ä‘Ã³:
- $(u, i, j)$: User $u$, positive item $i$, negative item $j$
- $\hat{x}_{ui}$: Predicted score cho positive item
- $\hat{x}_{uj}$: Predicted score cho negative item  
- $\sigma$: Sigmoid function

### Intuition

BPR loss tá»‘i Æ°u hÃ³a **pairwise ranking**:
- Positive item (user Ä‘Ã£ interact) nÃªn cÃ³ score **cao hÆ¡n** negative item
- Margin: $\hat{x}_{ui} - \hat{x}_{uj} > 0$

```
Positive: Item user clicked     â†’ score = 0.8
Negative: Random sampled item   â†’ score = 0.3
                                        â†“
BPR: maximize sigmoid(0.8 - 0.3) = sigmoid(0.5) â‰ˆ 0.62
Loss = -log(0.62) â‰ˆ 0.48
```

### Implementation

```python
def bpr_loss(pos_scores, neg_scores):
    """
    pos_scores: (batch_size,)
    neg_scores: (batch_size, num_negatives)
    """
    # Difference between positive and negative scores
    diff = pos_scores.unsqueeze(1) - neg_scores
    
    # Clamp for numerical stability
    diff = diff.clamp(-80, 80)
    
    # BPR loss
    loss = -torch.log(torch.sigmoid(diff) + 1e-10).mean()
    
    return loss
```

### Negative Sampling

Má»—i positive sample cÃ³ $k$ negative samples (default: $k=4$):

```python
def sample_negatives(positive_item, all_items, k=4):
    negatives = []
    while len(negatives) < k:
        neg = random.choice(all_items)
        if neg != positive_item:
            negatives.append(neg)
    return negatives
```

---

## ðŸ‡»ðŸ‡³ PhoBERT Integration

### Why PhoBERT?

- Pre-trained **Vietnamese language model** tá»« VinAI
- Hiá»ƒu semantic meaning cá»§a Vietnamese product descriptions
- Cold-start handling: New items cÃ³ thá»ƒ Ä‘Æ°á»£c represent qua text

### Architecture (Optional Enhancement)

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
User Sequence  â”€â”€â†’  â”‚    SASRec       â”‚ â”€â”€â†’ Sequence Repr
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼ (Fusion)
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
Item Text     â”€â”€â†’   â”‚    PhoBERT      â”‚ â”€â”€â†’ Content Repr
(Vietnamese)        â”‚   (Frozen)      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Fusion Strategies

1. **Concat**: `fused = Linear(concat(seq, content))`
2. **Gate**: `fused = gate * seq + (1-gate) * content`
3. **Addition**: `fused = proj(seq) + proj(content)`

---

## ðŸš€ HÆ°á»›ng dáº«n sá»­ dá»¥ng

### Quick Start

```bash
cd Newmethod

# Train (30 epochs, ~20-30 min on RTX 3060)
python main.py --mode train --epochs 30

# Quick test (2 epochs)
python main.py --mode train --epochs 2

# Evaluate
python main.py --mode evaluate

# Demo recommendations
python main.py --mode demo --user_id 28013
```

### API Usage

```python
from recommender import SASRecRecommender
from data_processor import TikiDataProcessor

# Load
processor = TikiDataProcessor()
processor.load_raw_data()

recommender = SASRecRecommender.load(
    'checkpoints/best_model.pt',
    processor
)

# Get recommendations for user
recs = recommender.recommend_for_user(user_id=12345, top_k=10)
for r in recs:
    print(f"{r['name']} - {r['price']} VND")

# Find similar items
similar = recommender.get_similar_items(item_id=277725874, top_k=5)
```

### Project Structure

```
Newmethod/
â”œâ”€â”€ main.py           # Entry point (train/eval/demo)
â”œâ”€â”€ config.py         # Configuration dataclasses
â”œâ”€â”€ data_processor.py # Load tiki_dataset.jsonl
â”œâ”€â”€ models.py         # SASRec + PhoBERT architecture
â”œâ”€â”€ trainer.py        # Training pipeline
â”œâ”€â”€ recommender.py    # Inference interface
â””â”€â”€ checkpoints/      # Saved models
```

---

## ðŸ“Š Expected Results

| Metric | 10 epochs | 30 epochs |
|--------|-----------|-----------|
| HR@10 | ~0.08-0.12 | ~0.15-0.25 |
| NDCG@10 | ~0.04-0.08 | ~0.10-0.18 |
| Training Time | ~8 min | ~25 min |

---

## ðŸ“– References

### Original Paper

1. **SASRec: Self-Attentive Sequential Recommendation**
   - Authors: Wang-Cheng Kang, Julian McAuley
   - Conference: ICDM 2018
   - Link: [https://arxiv.org/abs/1808.09781](https://arxiv.org/abs/1808.09781)
   - GitHub: [https://github.com/kang205/SASRec](https://github.com/kang205/SASRec)

### Related Papers

2. **Attention Is All You Need** (Transformer)
   - Authors: Vaswani et al.
   - Conference: NeurIPS 2017
   - Link: [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)

3. **BPR: Bayesian Personalized Ranking from Implicit Feedback**
   - Authors: Rendle et al.
   - Conference: UAI 2009
   - Link: [https://arxiv.org/abs/1205.2618](https://arxiv.org/abs/1205.2618)

4. **BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer**
   - Authors: Sun et al.
   - Conference: CIKM 2019
   - Link: [https://arxiv.org/abs/1904.06690](https://arxiv.org/abs/1904.06690)

5. **PhoBERT: Pre-trained language models for Vietnamese**
   - Authors: Nguyen & Nguyen (VinAI)
   - Conference: EMNLP 2020
   - Link: [https://arxiv.org/abs/2003.00744](https://arxiv.org/abs/2003.00744)

### Implementations

- RecBole (PyTorch): [https://recbole.io/](https://recbole.io/)
- SASRec PyTorch: [https://github.com/pmixer/SASRec.pytorch](https://github.com/pmixer/SASRec.pytorch)

---

## ðŸ“ Citation

```bibtex
@inproceedings{kang2018self,
  title={Self-Attentive Sequential Recommendation},
  author={Kang, Wang-Cheng and McAuley, Julian},
  booktitle={2018 IEEE International Conference on Data Mining (ICDM)},
  pages={197--206},
  year={2018},
  organization={IEEE}
}
```

---

## âš™ï¸ Requirements

- Python >= 3.8
- PyTorch >= 2.0
- transformers >= 4.30 (for PhoBERT)
- CUDA GPU recommended (tested on RTX 3060 12GB)

---

*Developed for Vietnamese E-commerce Recommendation System*
