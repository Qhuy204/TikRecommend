# ğŸ›ï¸ Intelligent E-commerce Product Recommendation System

> Há»‡ thá»‘ng gá»£i Ã½ sáº£n pháº©m thÃ´ng minh sá»­ dá»¥ng Deep Learning cho thÆ°Æ¡ng máº¡i Ä‘iá»‡n tá»­ Viá»‡t Nam

**Dataset:** TikDataset (Vietnamese E-commerce)  
**Architecture:** Two-Stage Funnel (Retrieval + Ranking)

---

## ğŸ“‹ Tá»•ng Quan

Há»‡ thá»‘ng nÃ y implement theo kiáº¿n trÃºc **Two-Stage Funnel** Ä‘Æ°á»£c sá»­ dá»¥ng bá»Ÿi cÃ¡c cÃ´ng ty lá»›n nhÆ° YouTube, TikTok, Shopee:

1. **Stage 1: Retrieval (Two-Tower Model)**
   - Lá»c nhanh tá»« hÃ ng triá»‡u sáº£n pháº©m â†’ ~100 á»©ng viÃªn
   - Sá»­ dá»¥ng PhoBERT Ä‘á»ƒ hiá»ƒu ngá»¯ nghÄ©a tiáº¿ng Viá»‡t
   - Giáº£i quyáº¿t Cold-start problem

2. **Stage 2: Ranking (MMoE Model)**
   - Sáº¯p xáº¿p 100 á»©ng viÃªn â†’ Top-N sáº£n pháº©m tá»‘t nháº¥t
   - Multi-task learning (Purchase, Quality, Price)
   - Tá»‘i Æ°u conversion rate

---

## ğŸš€ Quick Start

### 1. CÃ i Äáº·t Dependencies

```bash
# Clone repository
git clone <your-repo>
cd recommendation-system

# Install dependencies
pip install -r requirements.txt
```

**requirements.txt:**
```
torch>=2.0.0
transformers>=4.30.0
pandas>=2.0.0
numpy>=1.24.0
scikit-learn>=1.3.0
beautifulsoup4>=4.12.0
tqdm>=4.65.0
huggingface_hub>=0.16.0
```

### 2. Download Dataset

```bash
python download_dataset.py
```

Hoáº·c manual:
```python
from huggingface_hub import snapshot_download

snapshot_download(
    repo_id="Qhuy204/TikDataset",
    repo_type="dataset",
    local_dir="TikDataset",
    local_dir_use_symlinks=False
)
```

### 3. Cháº¡y Full Pipeline

```bash
# Option 1: Xá»­ lÃ½ tá»« JSONL file
python recommendation_system.py \
    --mode full \
    --raw_jsonl data/raw/tiki_dataset.jsonl \
    --clean_jsonl data/clean/tiki_dataset_clean.jsonl \
    --sample_size 1000

# Option 2: Xá»­ lÃ½ tá»« thÆ° má»¥c JSON files
python recommendation_system.py \
    --mode full \
    --data_dir TikDataset \
    --sample_size 1000
```

---

## ğŸ“ Cáº¥u TrÃºc ThÆ° Má»¥c

```
recommendation-system/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Dá»¯ liá»‡u thÃ´
â”‚   â”‚   â””â”€â”€ tiki_dataset.jsonl
â”‚   â”œâ”€â”€ clean/                  # Dá»¯ liá»‡u Ä‘Ã£ lÃ m sáº¡ch
â”‚   â”‚   â””â”€â”€ tiki_dataset_clean.jsonl
â”‚   â””â”€â”€ processed/              # Features Ä‘Ã£ xá»­ lÃ½
â”‚       â”œâ”€â”€ item_features.csv
â”‚       â”œâ”€â”€ ranking_features.csv
â”‚       â””â”€â”€ interactions.csv
â”œâ”€â”€ models/                     # Saved models
â”‚   â”œâ”€â”€ two_tower_best.pt
â”‚   â””â”€â”€ mmoe_best.pt
â”œâ”€â”€ recommendation_system.py    # Main preprocessing & models
â”œâ”€â”€ training_scripts.py         # Training utilities
â””â”€â”€ README.md
```

---

## ğŸ”§ Chi Tiáº¿t CÃ¡c BÆ°á»›c

### BÆ°á»›c 1: Data Cleaning

Loáº¡i bá» dá»¯ liá»‡u lá»—i vÃ  lÃ m sáº¡ch HTML:

```bash
# Clean only mode
python recommendation_system.py \
    --mode clean \
    --raw_jsonl data/raw/tiki_dataset.jsonl \
    --clean_jsonl data/clean/tiki_dataset_clean.jsonl
```

**CÃ¡c bÆ°á»›c xá»­ lÃ½:**
- âœ… Loáº¡i bá» products cÃ³ `error: redirect`
- âœ… Remove HTML tags tá»« descriptions
- âœ… Clean markdown vÃ  special characters
- âœ… Remove URLs, FAQs, footers
- âœ… Normalize whitespace

**Output:** `tiki_dataset_clean.jsonl` (chá»‰ chá»©a valid products)

### BÆ°á»›c 2: Feature Extraction

TrÃ­ch xuáº¥t features cho cáº£ 2 models:

```bash
# Preprocess only mode
python recommendation_system.py \
    --mode preprocess \
    --clean_jsonl data/clean/tiki_dataset_clean.jsonl
```

**Output:**

1. **item_features.csv** (cho Two-Tower):
   - `product_id`: ID sáº£n pháº©m
   - `text_content`: Tá»•ng há»£p text (name + description + specs)
   - `category`: Danh má»¥c
   - `brand_id`: ID thÆ°Æ¡ng hiá»‡u

2. **ranking_features.csv** (cho MMoE):
   - Dense features: `price`, `discount_rate`, `rating_average`, etc.
   - Sparse features: `seller_id`, `is_authentic`, `is_freeship`, etc.
   - Labels: `y_purchase`, `y_quality`, `y_price`

### BÆ°á»›c 3: Train Two-Tower Model (Retrieval)

```bash
python training_scripts.py --model two_tower --epochs 10 --batch_size 64
```

**YÃªu cáº§u:**
- File `data/processed/item_features.csv`
- File `data/processed/interactions.csv` (user-item interactions)

**Model Architecture:**
```
User Tower:
  UserID â†’ Embedding(64) â†’ Dense(128) â†’ L2 Norm

Item Tower:
  Text â†’ PhoBERT(768) â”€â”
  Category â†’ Embedding(32) â”€â”¼â”€â†’ Concat(832) â†’ Dense(128) â†’ L2 Norm
  Brand â†’ Embedding(32) â”€â”€â”€â”€â”˜

Loss: InfoNCE (Contrastive Learning)
```

**Metrics:** Recall@10, Recall@50

### BÆ°á»›c 4: Train MMoE Model (Ranking)

```bash
python training_scripts.py --model mmoe --epochs 20 --batch_size 256
```

**Model Architecture:**
```
Input Features (11D)
    â†“
[Expert 1] [Expert 2] [Expert 3] [Expert 4]
    â†“           â†“           â†“           â†“
Gate Purchase / Gate Quality / Gate Price
    â†“           â†“           â†“
Tower Purchase / Tower Quality / Tower Price
    â†“           â†“           â†“
  Ïƒ(buy)      Ïƒ(quality)  Ïƒ(price)
```

**Metrics:** AUC per task, Average AUC

---

## ğŸ“Š Dataset Schema

### Product JSON Structure

```json
{
  "product_id": 275257230,
  "category": "Trang trÃ­ nhÃ  cá»­a",
  "product_detail": {
    "name": "BÃ¬nh Hoa SÆ¡n MÃ i...",
    "description": "<p>MÃ´ táº£ sáº£n pháº©m...</p>",
    "price": 1790000,
    "rating_average": 4.5,
    "review_count": 100,
    "badges_new": [...],
    "current_seller": {...},
    "specifications": [...]
  },
  "reviews": [...]
}
```

### Feature Mapping

**Method 1 (Two-Tower) sá»­ dá»¥ng:**
- `name`, `description`, `short_description` â†’ PhoBERT embedding
- `specifications` â†’ Flatten text
- `category`, `brand.id` â†’ Categorical embeddings

**Method 2 (MMoE) sá»­ dá»¥ng:**
- **Dense:** `price`, `list_price`, `discount_rate`, `rating_average`, `review_count`, `quantity_sold`
- **Sparse:** `current_seller.id`, `is_authentic`, `is_freeship`, `has_return_policy`
- **Labels:** Extracted tá»« `reviews` vÃ  `vote_attributes`

---

## ğŸ¯ Experiment Plan (Cho BÃ¡o CÃ¡o Äá»“ Ãn)

### Experiment 1: Cold-Start Performance

**Má»¥c tiÃªu:** Chá»©ng minh Method 1 giáº£i quyáº¿t Cold-start tá»‘t hÆ¡n Baseline

**Setup:**
1. Split dataset: 70% train / 30% cold-start (sáº£n pháº©m má»›i, chÆ°a cÃ³ rating)
2. Baseline: Matrix Factorization (khÃ´ng dÃ¹ng content)
3. Method 1: Two-Tower vá»›i PhoBERT

**Metrics:**
- Recall@10, Recall@50
- Coverage (% sáº£n pháº©m Ä‘Æ°á»£c gá»£i Ã½)

**Expected Results:**
```
                 Recall@10  Recall@50  Coverage
Baseline (MF)      0.05       0.15      20%
Method 1 (Ours)    0.35       0.60      95%
```

### Experiment 2: Multi-Task Learning Power

**Má»¥c tiÃªu:** Chá»©ng minh Multi-task Learning cáº£i thiá»‡n Ranking

**Setup:**
1. Dataset: Sáº£n pháº©m cÃ³ Ä‘á»§ `vote_attributes` (quality, price signals)
2. Baseline: Single-task DNN (chá»‰ dá»± Ä‘oÃ¡n purchase)
3. Method 2: MMoE (3 tasks)

**Metrics:**
- AUC per task
- F1-score (Purchase task)

**Expected Results:**
```
                 AUC Purchase  AUC Quality  AUC Price  Avg AUC
Single-task DNN     0.72          -            -       0.72
Method 2 (MMoE)     0.78         0.75         0.74     0.76
```

---

## ğŸ”¬ Advanced Usage

### Extract User-Item Interactions

Náº¿u báº¡n cÃ³ reviews data:

```python
import pandas as pd
import json

interactions = []

with open('data/clean/tiki_dataset_clean.jsonl', 'r') as f:
    for line in f:
        data = json.loads(line)
        product_id = data['product_id']
        
        for review in data.get('reviews', []):
            user_id = review.get('customer_id')
            rating = review.get('rating', 0)
            
            if user_id and rating >= 4:
                interactions.append({
                    'user_id': user_id,
                    'product_id': product_id,
                    'rating': rating
                })

df = pd.DataFrame(interactions)
df.to_csv('data/processed/interactions.csv', index=False)
```

### Fine-tune PhoBERT

Náº¿u muá»‘n fine-tune PhoBERT trÃªn domain-specific data:

```python
from transformers import AutoModelForMaskedLM, Trainer

model = AutoModelForMaskedLM.from_pretrained('vinai/phobert-base')

# Fine-tune vá»›i product descriptions
# ... (setup dataset, trainer)
trainer.train()

# Save fine-tuned model
model.save_pretrained('models/phobert-ecommerce')
```

### Build Inference Pipeline

```python
from recommendation_system import TwoTowerModel, MMoEModel
import torch

# Load models
two_tower = TwoTowerModel(...)
two_tower.load_state_dict(torch.load('models/two_tower_best.pt'))

mmoe = MMoEModel(...)
mmoe.load_state_dict(torch.load('models/mmoe_best.pt')['model_state_dict'])

def recommend_for_user(user_id, top_k=10):
    # Stage 1: Retrieval
    user_emb = two_tower.user_tower(torch.tensor([user_id]))
    # ... (compute similarities vá»›i táº¥t cáº£ items)
    candidate_items = get_top_candidates(similarities, k=100)
    
    # Stage 2: Ranking
    features = extract_features(candidate_items)
    scores, _, _ = mmoe(features)
    
    # Return top-k
    return candidate_items[scores.topk(top_k).indices]
```

---

## ğŸ“ˆ Performance Benchmarks

### Hardware Requirements

**Minimum:**
- CPU: 4 cores
- RAM: 16GB
- Storage: 50GB

**Recommended:**
- GPU: NVIDIA RTX 3060 (12GB VRAM)
- RAM: 32GB
- Storage: 100GB SSD

### Training Time (1000 samples)

| Step | CPU | GPU (RTX 3060) |
|------|-----|----------------|
| Data Cleaning | 2 min | 2 min |
| Feature Extraction | 10 min | 8 min |
| Two-Tower (10 epochs) | 60 min | 15 min |
| MMoE (20 epochs) | 30 min | 5 min |

### Inference Time

| Operation | Latency |
|-----------|---------|
| Retrieval (100 candidates) | <50ms |
| Ranking (100 items) | <10ms |
| **End-to-end** | **<60ms** |

---

## ğŸ› Troubleshooting

### Error: "CUDA out of memory"

**Solution:**
```bash
# Giáº£m batch size
python training_scripts.py --batch_size 32

# Hoáº·c dÃ¹ng CPU
export CUDA_VISIBLE_DEVICES=-1
```

### Error: "PhoBERT download failed"

**Solution:**
```python
# Download manually trÆ°á»›c
from transformers import AutoTokenizer, AutoModel

AutoTokenizer.from_pretrained('vinai/phobert-base', cache_dir='./cache')
AutoModel.from_pretrained('vinai/phobert-base', cache_dir='./cache')
```

### Warning: "Synthetic labels created"

Náº¿u chÆ°a cÃ³ labels thá»±c tá»« reviews, system sáº½ táº¡o synthetic labels:
- `y_purchase`: rating >= 4
- `y_quality`: rating >= 4.5
- `y_price`: discount_rate > 20%

Äá»ƒ dÃ¹ng real labels, implement `extract_auxiliary_labels()` trong `TikDataPreprocessor`.

---

## ğŸ“š References

### Papers

1. **Two-Tower Models:**
   - Yi, X., et al. (2019). "Sampling-Bias-Corrected Neural Modeling for Large Corpus Item Recommendations". RecSys 2019.

2. **PhoBERT:**
   - Nguyen, D. Q., & Nguyen, A. T. (2020). "PhoBERT: Pre-trained language models for Vietnamese". EMNLP 2020.

3. **MMoE:**
   - Ma, J., et al. (2018). "Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts". KDD 2018.

### Code References

- Hugging Face Transformers: https://github.com/huggingface/transformers
- PhoBERT: https://github.com/VinAIResearch/PhoBERT
- TikDataset: https://huggingface.co/datasets/Qhuy204/TikDataset

---

## ğŸ“ Citation

Náº¿u sá»­ dá»¥ng code nÃ y trong nghiÃªn cá»©u, vui lÃ²ng cite:

```bibtex
@software{vietnamese_recsys_2024,
  title = {Intelligent E-commerce Product Recommendation System for Vietnamese Market},
  author = {Qhuy204},
  year = {2025},
  url = {https://github.com/Qhuy204}
}
```

---

## ğŸ“§ Contact & Support

- **Author:** Quoc Huy Truong
- **Email:** truongquochuy234@gmail.com
- **Issues:** [https://github.com/your-repo/issues](https://github.com/Qhuy204/TikRecommend/issues)

---

## ğŸ“„ License

MIT License - See LICENSE file for details.

---

**Built with â¤ï¸ for Vietnamese E-commerce**
